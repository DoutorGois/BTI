Retrieval-induced forgetting (RIF) refers to the finding that retrieving a memory can impair subsequent recall of related memories. Here, the authors present a new model of how the brain gives rise to RIF in both semantic and episodic memory. The core of the model is a recently developed neural network learning algorithm that leverages regular oscillations in feedback inhibition to strengthen weak parts of target memories and to weaken competing memories. The authors use the model to address several puzzling findings relating to RIF, including why retrieval practice leads to more forgetting than simply presenting the target item, how RIF is affected by the strength of competing memories and the strength of the target (to-be-retrieved) memory, and why RIF sometimes generalizes to independent cues and sometimes does not. For all of these questions, the authors show that the model can account for existing results, and they generate novel predictions regarding boundary conditions on these results.