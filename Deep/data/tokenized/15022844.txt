Despite the fact that animals are not optimal, natural selection is an optimizing process that can readily control small bits and pieces of organisms. It is for this reason that we need to explain certain parameters as found in Nature (e.g., number of neurons and their average activity) to fully understand the biological basis of cognition. In this optimizing sense, the failure of quantal synaptic transmission is problematic because this process incurs information loss at each synapse which seems like a bad thing for information processing. However, recent work based on an information-theoretic analysis of a single neuron suggests that such losses can be tolerated and lead to energy savings. Here we study computational simulations of a hippocampal model as a function of failure rate. We find that the failure process actually enhances some indices of performance when the model is required to solve the hippocampally dependent task of transverse patterning or when it is required to learn a simple sequence. Adding the random process of synaptic failures to the recurrent CA3-to-CA3 excitatory connections results in simulations that are more robust to parametric settings. Not only is the model more robust when synaptic failures are part of the model but there is a notable increase of sequence length memory capacity. Also, the failure process combined with additional neurons allows lower activity settings while still remaining compatible with learning the transverse patterning task. Indeed, as neuron number tended towards the biological numbers (nearly 5 x 10(4) in the simulations), it was not only possible to achieve biological failure rates (55-85%) at the minimally tolerated activity setting but these appropriately high failure rates were required for successful learning. The results are interpreted in terms of previous research demonstrating that randomization during training can enhance performance by facilitating implicit state-space search for interconnected neurons.