Effective navigation depends upon reliable estimates of head direction (HD). Visual, vestibular, and outflow motor signals combine for this purpose in a brain system that includes dorsal tegmental nucleus, lateral mammillary nuclei, anterior dorsal thalamic nucleus, and the postsubiculum. Learning is needed to combine such different cues to provide reliable estimates of HD. A neural model is developed to explain how these three types of signals combine adaptively within the above brain regions to generate a consistent and reliable HD estimate, in both light and darkness, which explains the following experimental facts. Each HD cell is tuned to a preferred head direction. The cell's firing rate is maximal at the preferred direction and decreases as the head turns from the preferred direction. The HD estimate is controlled by the vestibular system when visual cues are not available. A well-established visual cue anchors the cell's preferred direction when the cue is in the animal's field of view. Distal visual cues are more effective than proximal cues for anchoring the preferred direction. The introduction of novel cues in either a novel or familiar environment can gain control over a cell's preferred direction within minutes. Turning out the lights or removing all familiar cues does not change the cell's firing activity, but it may accumulate a drift in the cell's preferred direction. The anticipated time interval (ATI) of the HD estimate is greater in early processing stages of the HD system than at later stages. The model contributes to an emerging unified neural model of how multiple processing stages in spatial navigation, including postsubiculum head direction cells, entorhinal grid cells, and hippocampal place cells, are calibrated through learning in response to multiple types of signals as an animal navigates in the world.