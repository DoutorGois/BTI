{
     "PMID": "12790185",
     "OWN": "NLM",
     "STAT": "MEDLINE",
     "DCOM": "20030716",
     "LR": "20061115",
     "IS": "0954-898X (Print) 0954-898X (Linking)",
     "VI": "14",
     "IP": "2",
     "DP": "2003 May",
     "TI": "Network capacity analysis for latent attractor computation.",
     "PG": "273-302",
     "AB": "Attractor networks have been one of the most successful paradigms in neural computation, and have been used as models of computation in the nervous system. Recently, we proposed a paradigm called 'latent attractors' where attractors embedded in a recurrent network via Hebbian learning are used to channel network response to external input rather than becoming manifest themselves. This allows the network to generate context-sensitive internal codes in complex situations. Latent attractors are particularly helpful in explaining computations within the hippocampus--a brain region of fundamental significance for memory and spatial learning. Latent attractor networks are a special case of associative memory networks. The model studied here consists of a two-layer recurrent network with attractors stored in the recurrent connections using a clipped Hebbian learning rule. The firing in both layers is competitive--K winners take all firing. The number of neurons allowed to fire, K, is smaller than the size of the active set of the stored attractors. The performance of latent attractor networks depends on the number of such attractors that a network can sustain. In this paper, we use signal-to-noise methods developed for standard associative memory networks to do a theoretical and computational analysis of the capacity and dynamics of latent attractor networks. This is an important first step in making latent attractors a viable tool in the repertoire of neural computation. The method developed here leads to numerical estimates of capacity limits and dynamics of latent attractor networks. The technique represents a general approach to analyse standard associative memory networks with competitive firing. The theoretical analysis is based on estimates of the dendritic sum distributions using Gaussian approximation. Because of the competitive firing property, the capacity results are estimated only numerically by iteratively computing the probability of erroneous firings. The analysis contains two cases: the simple case analysis which accounts for the correlations between weights due to shared patterns and the detailed case analysis which includes also the temporal correlations between the network's present and previous state. The latter case predicts better the dynamics of the network state for non-zero initial spurious firing. The theoretical analysis also shows the influence of the main parameters of the model on the storage capacity.",
     "FAU": [
          "Doboli, Simona",
          "Minai, Ali A"
     ],
     "AU": [
          "Doboli S",
          "Minai AA"
     ],
     "AD": "Computer Science Department, Hofstra University, Hempstead, NY 11549, USA. cscszd@hofstra.edu",
     "LA": [
          "eng"
     ],
     "PT": [
          "Journal Article",
          "Research Support, U.S. Gov't, Non-P.H.S."
     ],
     "PL": "England",
     "TA": "Network",
     "JT": "Network (Bristol, England)",
     "JID": "9431867",
     "SB": "IM",
     "MH": [
          "Animals",
          "Association Learning/*physiology",
          "Brain Mapping",
          "Dendrites/physiology",
          "Hippocampus/cytology/*physiology",
          "Memory/physiology",
          "*Neural Networks (Computer)",
          "Space Perception/physiology"
     ],
     "EDAT": "2003/06/07 05:00",
     "MHDA": "2003/07/17 05:00",
     "CRDT": [
          "2003/06/07 05:00"
     ],
     "PHST": [
          "2003/06/07 05:00 [pubmed]",
          "2003/07/17 05:00 [medline]",
          "2003/06/07 05:00 [entrez]"
     ],
     "PST": "ppublish",
     "SO": "Network. 2003 May;14(2):273-302.",
     "term": "hippocampus"
}